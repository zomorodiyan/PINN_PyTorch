{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bcf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] normalization/preprocessing/scaling\n",
    "# [] visualization/tensorboard-histograms\n",
    "# [] \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98e2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    all_snapshots = []\n",
    "    lines_per_snapshot = 22341\n",
    "    lines_of_garbage = 6\n",
    "    lines_per_section = lines_per_snapshot + lines_of_garbage\n",
    "    column_names = ['p', 'c', 'w', 'k', 'u', 'v', 'x', 'y']\n",
    "    snapshot_number = 0\n",
    "    for chunk in pd.read_csv(file_path, \n",
    "                             chunksize=lines_per_section,\n",
    "                             skip_blank_lines=False,\n",
    "                             names=column_names,\n",
    "                             delimiter=','):       \n",
    "        snapshot_number += 1\n",
    "        chunk['snapshot'] = None\n",
    "        data_lines = chunk.iloc[lines_of_garbage:].copy()\n",
    "        if (len(data_lines)==0):break\n",
    "        print(f\"Loading snapshot {snapshot_number}/100, Length: {len(data_lines)}\", end='\\r', flush=True)\n",
    "        data_lines['t'] = snapshot_number\n",
    "        all_snapshots.append(data_lines)\n",
    "    all_data = pd.concat(all_snapshots, ignore_index=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data using the load_csv function\n",
    "data = load_csv('your_data.csv')\n",
    "\n",
    "# Split the data into train and test sets (you can adjust the test_size)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35721d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Dataset for your data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, input_scaler=None, output_scaler=None):\n",
    "        self.data = data\n",
    "        self.input_scaler = input_scaler\n",
    "        self.output_scaler = output_scaler\n",
    "        self.x = torch.tensor(data[['x', 'y', 't', 'wind', 'leak_x', 'leak_y', 'leak_s']].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(data[['u', 'v', 'c', 'k', 'w']].values, dtype=torch.float32)\n",
    "        \n",
    "        if input_scaler is not None:\n",
    "            self.x = input_scaler.transform(self.x)\n",
    "        \n",
    "        if output_scaler is not None:\n",
    "            self.y[:, :2] = output_scaler.transform(self.y[:, 2:])\n",
    "            \n",
    "            # Logarithmic scaling for 'c'\n",
    "            self.y[:, 2] = torch.log1p(self.y[:, 2])  # Apply log1p to avoid log(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test datasets with feature scaling and logarithmic scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize input and output scalers\n",
    "input_scaler = StandardScaler()\n",
    "output_scaler = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data\n",
    "input_scaler.fit(train_data[['x', 'y', 't', 'wind', 'leak_x', 'leak_y', 'leak_s']].values)\n",
    "output_scaler.fit(train_data[['u', 'v', 'c', 'k', 'w']].values)\n",
    "\n",
    "# Create datasets with scalers\n",
    "train_dataset = MyDataset(train_data, input_scaler, output_scaler)\n",
    "test_dataset = MyDataset(test_data, input_scaler, output_scaler)\n",
    "\n",
    "# Create DataLoader instances for batching\n",
    "batch_size = 32  # You can adjust this based on your needs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ea4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzomo\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8990346193313599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(7, 20))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        for _ in range(7):\n",
    "            self.layers.append(nn.Linear(20, 20))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.output_layer = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "net = MyNet()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Dummy input data (you should replace this with your actual data)\n",
    "input_data = torch.randn(1, 7, requires_grad=True)  # Requires gradients\n",
    "\n",
    "# Forward pass through the network\n",
    "outputs = net(input_data)\n",
    "\n",
    "# Calculate derivatives element-wise\n",
    "u_x = torch.autograd.grad(outputs[:, 0].sum(), input_data, create_graph=True)[0]\n",
    "v_y = torch.autograd.grad(outputs[:, 1].sum(), input_data, create_graph=True)[0]\n",
    "c_x = torch.autograd.grad(outputs[:, 2].sum(), input_data, create_graph=True)[0]  # Updated\n",
    "c_y = torch.autograd.grad(outputs[:, 3].sum(), input_data, create_graph=True)[0]  # Updated\n",
    "c_t = torch.autograd.grad(outputs[:, 4].sum(), input_data, create_graph=True)[0]  # Updated\n",
    "c_xx = torch.autograd.grad(c_x[:, 0].sum(), input_data, create_graph=True)[0]  # Updated\n",
    "c_yy = torch.autograd.grad(c_y[:, 1].sum(), input_data, create_graph=True)[0]  # Updated\n",
    "\n",
    "# Calculate continuity and convection functions\n",
    "continuity = u_x[:, 0] + v_y[:, 1]\n",
    "convection = (outputs[:, 0] * u_x[:, 0] + outputs[:, 1] * v_y[:, 1] +\n",
    "              outputs[:, 2] * c_x[:, 0] + outputs[:, 3] * c_y[:, 1] +\n",
    "              outputs[:, 4] * c_t[:, 2] - c_xx[:, 0] - c_yy[:, 1])\n",
    "\n",
    "# Define target values (you should replace this with your target data)\n",
    "u_pred = torch.randn(1, 1)\n",
    "v_pred = torch.randn(1, 1)\n",
    "c_pred = torch.randn(1, 1)\n",
    "k_pred = torch.randn(1, 1)\n",
    "w_pred = torch.randn(1, 1)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(outputs[:, 0], u_pred) + \\\n",
    "       criterion(outputs[:, 1], v_pred) + \\\n",
    "       criterion(outputs[:, 2], c_pred) + \\\n",
    "       criterion(outputs[:, 3], k_pred) + \\\n",
    "       criterion(outputs[:, 4], w_pred) + \\\n",
    "       criterion(continuity, torch.zeros_like(continuity)) + \\\n",
    "       criterion(convection, torch.zeros_like(convection))\n",
    "\n",
    "# Backpropagation and optimization (you should replace this with your training loop)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Print the loss\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network\n",
    "net = MyNet()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (you should replace this with your actual training loop)\n",
    "epochs = 10  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the test set (you should replace this with your evaluation code)\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = net(inputs)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Test Loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# Save your trained model if needed\n",
    "# torch.save(net.state_dict(), 'my_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a17c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define your load_csv function here\n",
    "def load_csv(file_path):\n",
    "    # Your implementation to load data into a Pandas DataFrame\n",
    "    # ...\n",
    "\n",
    "# Load your data using the load_csv function\n",
    "data = load_csv('your_data.csv')  # Replace 'your_data.csv' with your actual data file path\n",
    "\n",
    "# Split the data into train and test sets (you can adjust the test_size)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom PyTorch Dataset for your data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, input_scaler=None, output_scaler=None):\n",
    "        self.data = data\n",
    "        self.input_scaler = input_scaler\n",
    "        self.output_scaler = output_scaler\n",
    "        self.x = torch.tensor(data[['x', 'y', 't', 'wind', 'leak_x', 'leak_y', 'leak_s']].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(data[['u', 'v', 'c', 'k', 'w']].values, dtype=torch.float32)\n",
    "        \n",
    "        if input_scaler is not None:\n",
    "            self.x = input_scaler.transform(self.x)\n",
    "        \n",
    "        if output_scaler is not None:\n",
    "            self.y[:, :2] = output_scaler.transform(self.y[:, :2])\n",
    "            # Logarithmic scaling for 'c'\n",
    "            self.y[:, 2] = torch.log1p(self.y[:, 2])  # Apply log1p to avoid log(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create train and test datasets with feature scaling and logarithmic scaling\n",
    "# Initialize input and output scalers\n",
    "input_scaler = StandardScaler()\n",
    "output_scaler = StandardScaler()\n",
    "\n",
    "# Fit scalers on training data\n",
    "input_scaler.fit(train_data[['x', 'y', 't', 'wind', 'leak_x', 'leak_y', 'leak_s']].values)\n",
    "output_scaler.fit(train_data[['u', 'v', 'c', 'k', 'w']].values)\n",
    "\n",
    "# Create datasets with scalers\n",
    "train_dataset_scaled = MyDataset(train_data, input_scaler, output_scaler)\n",
    "test_dataset_scaled = MyDataset(test_data, input_scaler, output_scaler)\n",
    "\n",
    "# Create DataLoader instances for batching with scaled data\n",
    "batch_size = 32  # You can adjust this based on your needs\n",
    "train_loader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size, shuffle=True)\n",
    "test_loader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create datasets without scalers for unscaled loss calculation\n",
    "train_dataset_unscaled = MyDataset(train_data)\n",
    "test_dataset_unscaled = MyDataset(test_data)\n",
    "\n",
    "# Create DataLoader instances for batching with unscaled data\n",
    "train_loader_unscaled = DataLoader(train_dataset_unscaled, batch_size=batch_size, shuffle=True)\n",
    "test_loader_unscaled = DataLoader(test_dataset_unscaled, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network architecture (same as before)\n",
    "class MyNet(nn.Module):\n",
    "    # ...\n",
    "\n",
    "# Instantiate the model\n",
    "net = MyNet()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (you should replace this with your actual training loop)\n",
    "epochs = 10  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    for batch_scaled, batch_unscaled in zip(train_loader_scaled, train_loader_unscaled):\n",
    "        inputs_scaled, targets_scaled = batch_scaled\n",
    "        inputs_unscaled, targets_unscaled = batch_unscaled\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with scaled inputs\n",
    "        outputs_scaled = net(inputs_scaled)\n",
    "        \n",
    "        # Inverse transformations for unscaled predictions\n",
    "        outputs_unscaled = torch.clone(outputs_scaled)\n",
    "        outputs_unscaled[:, :2] = output_scaler.inverse_transform(outputs_unscaled[:, :2])\n",
    "        outputs_unscaled[:, 2] = torch.exp(outputs_unscaled[:, 2]) - 1  # Inverse log transformation for 'c'\n",
    "        \n",
    "        # Compute derivatives and other terms needed for loss calculation\n",
    "        # (You can use the unscaled predictions and inputs here)\n",
    "        u_x_unscaled = ...  # Calculate u_x with unscaled data\n",
    "        v_y_unscaled = ...  # Calculate v_y with unscaled data\n",
    "        c_x_unscaled = ...  # Calculate c_x with unscaled data\n",
    "        c_y_unscaled = ...  # Calculate c_y with unscaled data\n",
    "        c_t_unscaled = ...  # Calculate c_t with unscaled data\n",
    "        c_xx_unscaled = ...  # Calculate c_xx with unscaled data\n",
    "        c_yy_unscaled = ...  # Calculate c_yy with unscaled data\n",
    "        \n",
    "        # Compute continuity and convection functions with unscaled data\n",
    "        continuity_unscaled = u_x_unscaled + v_y_unscaled\n",
    "        convection_unscaled = (outputs_unscaled[:, 0] * u_x_unscaled + outputs_unscaled[:, 1] * v_y_unscaled +\n",
    "                               outputs_unscaled[:, 2] * c_x_unscaled + outputs_unscaled[:, 3] * c_y_unscaled +\n",
    "                               outputs_unscaled[:, 4] * c_t_unscaled - c_xx_unscaled - c_yy_unscaled)\n",
    "        \n",
    "        # Calculate the loss with unscaled data\n",
    "        loss = criterion(outputs_unscaled[:, 0], targets_unscaled[:, 0]) + \\\n",
    "               criterion(outputs_unscaled[:, 1], targets_unscaled[:, 1]) + \\\n",
    "               criterion(outputs_unscaled[:, 2], targets_unscaled[:, 2]) + \\\n",
    "               criterion(outputs_unscaled[:, 3], targets_unscaled[:, 3]) + \\\n",
    "               criterion(outputs_unscaled[:, 4], targets_unscaled[:, 4]) + \\\n",
    "               criterion(continuity_unscaled, torch.zeros_like(continuity_unscaled)) + \\\n",
    "               criterion(convection_unscaled, torch.zeros_like(convection_unscaled))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the test set (you should replace this with your evaluation code)\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        for batch_scaled, batch_unscaled in zip(test_loader_scaled, test_loader_unscaled):\n",
    "            inputs_scaled, targets_scaled = batch_scaled\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
